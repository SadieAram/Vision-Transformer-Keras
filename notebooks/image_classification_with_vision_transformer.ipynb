{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be-AbM4aG3wJ"
      },
      "source": [
        "# Image classification with Vision Transformer\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/18<br>\n",
        "**Last modified:** 2021/01/18<br>\n",
        "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfefsWlBG3wP"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
        "model by Alexey Dosovitskiy et al. for image classification,\n",
        "and demonstrates it on the CIFAR-100 dataset.\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
        "image patches, without using convolution layers.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
        "which can be installed using the following command:\n",
        "\n",
        "```python\n",
        "pip install -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNq3xn5KG3wQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons[tensorflow]"
      ],
      "metadata": {
        "id": "4KF5WHbpL3JV",
        "outputId": "99d5ee39-bf94-4c79-b357-4cec95f90578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons[tensorflow] in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons[tensorflow]) (4.1.5)\n",
            "Collecting tensorflow<2.9.0,>=2.6.0 (from tensorflow-addons[tensorflow])\n",
            "  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.9.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.3.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8 (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.59.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow])\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9.0,>=2.6.0->tensorflow-addons[tensorflow]) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, keras, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.4 tensorflow-estimator-2.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JSV2JRYqG3wR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F3slWMsG3wT"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0B1s2xzSG3wT",
        "outputId": "f54783a7-bb3e-4c0a-e9c9-e145f5745383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULKaYBSG3wU"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VnCqQbZKG3wU"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygn9wMU9G3wV"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0X-mnVa6G3wW"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpDk8HRpG3wW"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EGHti9GKG3wX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IehtYx-gG3wX"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sBZkdf65G3wY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHUrigKwG3wY"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Upo8e5p_G3wZ",
        "outputId": "f018c5e2-ce5a-4609-990e-76d967155a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKaE1czAG3wZ"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XlpuXCISG3wa"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fL76d5dG3wb"
      },
      "source": [
        "## Build the ViT model\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks,\n",
        "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
        "applied to the sequence of patches. The Transformer blocks produce a\n",
        "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
        "classifier head with softmax to produce the final class probabilities output.\n",
        "\n",
        "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
        "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
        "as the image representation, all the outputs of the final Transformer block are\n",
        "reshaped with `layers.Flatten()` and used as the image\n",
        "representation input to the classifier head.\n",
        "Note that the `layers.GlobalAveragePooling1D` layer\n",
        "could also be used instead to aggregate the outputs of the Transformer block,\n",
        "especially when the number of patches and the projection dimensions are large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L6jV0jhbG3wb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo5A7BU0G3wb"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "29p_JuAKG3wc",
        "outputId": "6a27c41f-f676-40d5-c343-df7feec3af7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 58/176 [========>.....................] - ETA: 42s - loss: 4.8634 - accuracy: 0.0242 - top-5-accuracy: 0.0949"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6ee34a4aff67>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-6ee34a4aff67>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_a7gMmMG3wc"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
        "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
        "\n",
        "Note that the state of the art results reported in the\n",
        "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
        "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
        "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
        "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions.\n",
        "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices,\n",
        "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
        "In practice, it's recommended to fine-tune a ViT model\n",
        "that was pre-trained using a large, high-resolution dataset."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classification_with_vision_transformer",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}